{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CSC2515_NNwithPyTorch_2021_blank.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KOBD_7jcEww6"
      },
      "source": [
        "Why use `PyTorch`? `PyTorch` has two main advantages: \n",
        "\n",
        "\n",
        "*   You can perform NumPy operations on a GPU (could be decrease runtime)\n",
        "*   It has automatic differention, which enables easier model training\n",
        "\n",
        "Also, `PyTorch` is often considered to be intuitive than other frameworks (e.g. Tensorflow 1) because it follows the structure of common Python practices more. \n",
        "\n",
        "You can load `PyTorch` into your Python environment with the following line: \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOjHyOlHCXj-"
      },
      "source": [
        "# Neural Networks with `PyTorch` ðŸ”¥\n",
        "### Tutorial for CSC2515, Fall 2021\n",
        "###Author: Marta Skreta\n",
        "\n",
        "In this notebook, I will give an introduction to `PyTorch` and how to get started on training models. This tutorial is adapted from the following sources:\n",
        "\n",
        "\n",
        "*   [60 Minute Blitz with PyTorch](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html) \n",
        "*   Andrew Ng's [Introduction to PyTorch Examples](https://cs230.stanford.edu/blog/pytorch/) for CS230 at Stanford University\n",
        "*   Dilara Soylu's [PyTorch Tutorial](https://web.stanford.edu/class/cs224n/materials/CS224N_PyTorch_Tutorial.html) for CS224 at Stanford University\n",
        "\n",
        "I highly recommend looking at them to get a better understanding of `PyTorch` basics.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8H1f5FDJQ9Uu"
      },
      "source": [
        "import torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16LLOZPvNfa7"
      },
      "source": [
        "The basic data structure used in `PyTorch` is a `tensor`. This is very similar to a NumPy `ndarray`, except that you can run them on GPUs.  \n",
        "\n",
        "You can initialize `tensors` in a few different ways:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hjOvT3R8QbY-"
      },
      "source": [
        "### A few ways to initialize a  `tensor`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XOWDl1oQmS2"
      },
      "source": [
        "> 1. Directly from the data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "beroe9MaC7GD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75668205-acb9-4a46-95f9-a00bd7b6c31c"
      },
      "source": [
        "x = [[0,1], [2,3]]\n",
        "x_tensor = torch.tensor(x)\n",
        "\n",
        "print(f\"Tensor from data list: \\n {x_tensor} \\n\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensor from data list: \n",
            " tensor([[0, 1],\n",
            "        [2, 3]]) \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFJJSz6ER6y8"
      },
      "source": [
        "\n",
        "\n",
        "> 2. From a `NumPy` array:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-dCODh3R-zc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b4a16a7-aff6-4af9-edeb-0d0b2d9f71e9"
      },
      "source": [
        "import numpy as np\n",
        "x = np.array(x)\n",
        "x_tensor = torch.from_numpy(x)\n",
        "\n",
        "print(f\"Tensor from NumPy: \\n {x_tensor} \\n\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensor from NumPy: \n",
            " tensor([[0, 1],\n",
            "        [2, 3]]) \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fFNnnKadUp9N"
      },
      "source": [
        "\n",
        "> 3. With random or constant values:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6gDaP7oqUuIX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f36dea79-2408-42e1-fd5e-77903ed34280"
      },
      "source": [
        "shape = (2,3)\n",
        "rand_tensor = torch.rand(shape)\n",
        "ones_tensor = torch.ones(shape)\n",
        "zeros_tensor = torch.zeros(shape)\n",
        "\n",
        "print(f\"Random Tensor: \\n {rand_tensor} \\n\")\n",
        "print(f\"Ones Tensor: \\n {ones_tensor} \\n\")\n",
        "print(f\"Zeros Tensor: \\n {zeros_tensor}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Tensor: \n",
            " tensor([[0.5993, 0.9360, 0.8067],\n",
            "        [0.4213, 0.9121, 0.8752]]) \n",
            "\n",
            "Ones Tensor: \n",
            " tensor([[1., 1., 1.],\n",
            "        [1., 1., 1.]]) \n",
            "\n",
            "Zeros Tensor: \n",
            " tensor([[0., 0., 0.],\n",
            "        [0., 0., 0.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Myd6p-bHXG2z"
      },
      "source": [
        "## Moving a `Tensor` onto the GPU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kAgGB3xBXJ0M"
      },
      "source": [
        "I've already mentioned a few times that a huge benefit of `PyTorch` is that you can perform data operations on the GPU. So how do you do that? PyTorch doesn't automatically move `tensors` onto the GPU if you have one, so you have to indicate that in your code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VyNTe-6FXfao",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "977a4a13-7a2b-461f-f847-c6767237faf7"
      },
      "source": [
        "x = [[0, 1, 2], [3, 4, 5]]\n",
        "x_tensor = torch.tensor(x)\n",
        "print(f\"Device tensor is stored on: {x_tensor.device}\")\n",
        "\n",
        "# We move our tensor to the GPU if available [in Colab, you can load a GPU by going into Edit > Notebook settings]\n",
        "if torch.cuda.is_available():\n",
        "  x_tensor = x_tensor.to('cuda')\n",
        "  print(f\"Device tensor is stored on: {x_tensor.device}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device tensor is stored on: cpu\n",
            "Device tensor is stored on: cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZRSGQffBULiV"
      },
      "source": [
        "## `Tensor` Operations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KviTIeA2WZXh"
      },
      "source": [
        "There are **a lot** of operations you can perform on `tensors` (like with NumPy arrays). I've only listed a few below, but the fill list can be found [here](https://pytorch.org/docs/stable/torch.html). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_jgL4driW_as",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41b34e24-2303-4123-e8d7-d34b40a21be7"
      },
      "source": [
        "# Taking the mean\n",
        "x = torch.tensor([[1., 1.],\n",
        "                  [2., 2.],\n",
        "                  [3., 3.],\n",
        "                  [4., 4.]])\n",
        "\n",
        "print(f\"Mean: {x.mean()}\")\n",
        "print(f\"Mean over the columns: {x.mean(0)}\")\n",
        "print(f\"Mean over the rows: {x.mean(1)}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean: 2.5\n",
            "Mean over the columns: tensor([2.5000, 2.5000])\n",
            "Mean over the rows: tensor([1., 2., 3., 4.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0PGy2rbZx43",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c882a2f-166d-4b58-a1c3-b368130528ab"
      },
      "source": [
        "# Concatenating \n",
        "x_cat0 = torch.cat([x, x, x], dim=0)\n",
        "x_cat1 = torch.cat([x, x, x], dim=1)\n",
        "\n",
        "print(f\"Initial shape: {x.shape}\")\n",
        "print(f\"Shape after concatenation in dimension 0: {x_cat0.shape}\")\n",
        "print(f\"Shape after concatenation in dimension 1: {x_cat1.shape}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial shape: torch.Size([4, 2])\n",
            "Shape after concatenation in dimension 0: torch.Size([12, 2])\n",
            "Shape after concatenation in dimension 1: torch.Size([4, 6])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHNvKF2WbRrp"
      },
      "source": [
        "## Autograd\n",
        "\n",
        "`PyTorch` allows you to perform automatic differentiaion on a tensor. If you specify a set of operations, `PyTorch` builds a graph behind the scenes of what variables depend on each other. Then, when you use the `backward()` method, `PyTorch` automatically computes the gradients for you!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0f-867cSbqnh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42bc01bb-31a5-4e76-ba8d-5fb980282e09"
      },
      "source": [
        "# requires_grad is a parameter that indicates whether we want to compute the gradient for a given tensor\n",
        "x = torch.tensor([2.], requires_grad=True)\n",
        "print(f\"Current gradient of x: {x.grad}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current gradient of x: None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bj_H9b9vcVT5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6674ea4-0631-49f6-afbc-055ab17217c2"
      },
      "source": [
        "# now let's set up the following operation: y = 3x^2\n",
        "y = 3 * x * x\n",
        "print(y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([12.], grad_fn=<MulBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2RhAUNXJcefm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        },
        "outputId": "c7925d35-6729-4506-e136-f82b9de2f8dc"
      },
      "source": [
        "# let's compute the gradient of y wrt x:\n",
        "y.backward()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-880333314bfc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# let's compute the gradient of y wrt x:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    154\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FT4W19Lvcxe8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "756dcbfd-0743-427a-ddf5-cfc7627e59a2"
      },
      "source": [
        "# now let's view the gradient values:\n",
        "print(f\"Gradient of y wrt x: {x.grad}\")\n",
        "\n",
        "## checking it by hand, we have dy/dx = 6x = 6*2 = 12, which matches the answer!"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient of y wrt x: tensor([12.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LSw9hbxkgovT"
      },
      "source": [
        "z = 2*x\n",
        "z.backward()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wkUTCszygulF",
        "outputId": "6dc3afbe-417f-4e20-ed45-09ec284bc29e"
      },
      "source": [
        "print(f\"Current gradient of x: {x.grad}\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current gradient of x: tensor([14.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xbr_-N4fdspp"
      },
      "source": [
        "## Training a Neural Network\n",
        "\n",
        "There are 5 core steps for train a neural network:\n",
        "\n",
        "\n",
        "1.   Passing the data through the model\n",
        "2.   Computing a loss\n",
        "3.   Clearing the previous gradients (why is it important to do this\")\n",
        "4.   Computing the gradients of all variables wrt loss\n",
        "5.   Update model parameters based on gradients\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUeJSWTNesvo"
      },
      "source": [
        " > 1. Passing data through a model\n",
        "\n",
        "\n",
        " First, let's learn how to set up a model. We will use predefined building blocks in the `torch.nn` module of `PyTorch`, which we can later use to build more complicated models. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ou1K9ZaYfGDw"
      },
      "source": [
        "import torch.nn as nn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-H9HVPYfu8U"
      },
      "source": [
        "We can use `nn.Linear(D_in, D_out)` to create a linear layer. This will take a matrix of dimension (N, D_in) and output a matrix of dimension (N, D_out). The linear layer performs the operation `Wx+b`, where `W` is the weight matrix, `x` is the model input, and `b` is the bias. If you don't want your model to learn a bias, you can set `bias=False`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s3BmMhHxgPcT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca07c057-ce3e-417b-ade1-b0aa5d2e58b8"
      },
      "source": [
        "# model input where (N, D_in) = (10, 4) --> ten samples, each of 4 dimensions\n",
        "input = torch.ones(10, 4)\n",
        "# Make a linear layer transforming the number of dimensions from 4 to 2\n",
        "# Notice that we don't care how many samples we are passing in, we just care \n",
        "# what dimensions each sample will have before and after being passed through the layer\n",
        "linear = nn.Linear(4,2)\n",
        "output = linear(input)\n",
        "\n",
        "print(f\"Model output: {output}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model output: tensor([[0.1611, 0.9501],\n",
            "        [0.1611, 0.9501],\n",
            "        [0.1611, 0.9501],\n",
            "        [0.1611, 0.9501],\n",
            "        [0.1611, 0.9501],\n",
            "        [0.1611, 0.9501],\n",
            "        [0.1611, 0.9501],\n",
            "        [0.1611, 0.9501],\n",
            "        [0.1611, 0.9501],\n",
            "        [0.1611, 0.9501]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSlzyLfFhXQA"
      },
      "source": [
        "There are a number of `torch.nn` modules you can use apart from `nn.Linear`, such as `nn.LSTM`, `nn.Conv2d`, `nn.MaxPool2d`, `nn.BatchNorm1d`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VR4rr9uPhyJO"
      },
      "source": [
        "You can apply non-linear activations to your tensors. Some common ones include: `nn.ReLU()`, `nn.Sigmoid()` and `nn.LeakyReLU()`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-nOaGQMiDWQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "caae953b-54ae-4dde-c4d9-31ec11a18da0"
      },
      "source": [
        "sigmoid = nn.Sigmoid()\n",
        "output_sigmoid = sigmoid(output)\n",
        "\n",
        "print(f\"Output after passing through non-linearity: {output_sigmoid}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output after passing through non-linearity: tensor([[0.5402, 0.7211],\n",
            "        [0.5402, 0.7211],\n",
            "        [0.5402, 0.7211],\n",
            "        [0.5402, 0.7211],\n",
            "        [0.5402, 0.7211],\n",
            "        [0.5402, 0.7211],\n",
            "        [0.5402, 0.7211],\n",
            "        [0.5402, 0.7211],\n",
            "        [0.5402, 0.7211],\n",
            "        [0.5402, 0.7211]], grad_fn=<SigmoidBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45Hbj0ihiVSe"
      },
      "source": [
        "We can also combine `PyTorch` modules into a single block using `nn.Sequential`. That way, when we pass our data into the block in one single step, and it will performs all our operations. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBm8jNPxiiDi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eefe679a-7ea3-4b9b-8091-d988ce04166f"
      },
      "source": [
        "# Here, we are making a model block of one linear layer and one sigmoid activation\n",
        "\n",
        "model = nn.Sequential(nn.Linear(4,2), nn.Sigmoid())\n",
        "output = model(input)\n",
        "print(f\"Model output: {output}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model output: tensor([[0.6118, 0.2648],\n",
            "        [0.6118, 0.2648],\n",
            "        [0.6118, 0.2648],\n",
            "        [0.6118, 0.2648],\n",
            "        [0.6118, 0.2648],\n",
            "        [0.6118, 0.2648],\n",
            "        [0.6118, 0.2648],\n",
            "        [0.6118, 0.2648],\n",
            "        [0.6118, 0.2648],\n",
            "        [0.6118, 0.2648]], grad_fn=<SigmoidBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GhZb9If8mkia"
      },
      "source": [
        "Defining a Custom Model\n",
        "\n",
        "We can build custom models by extending the `nn.Module` class. This will allow us to build more complicated models or add custom features. First, you define your model class by extending the `nn.Module` class. Then, you have to implement two methods: (1) the `__init__()` method and (2) the `forward()` method.\n",
        "\n",
        "\n",
        "\n",
        "1.   the `__init__()` method constructs the architecture of the model at start-time\n",
        "2.   the `forward()` method constructs the forward pass through the model on a batch of your data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-nkG8broGAL"
      },
      "source": [
        "class CustomModel(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, output_size):\n",
        "    super(CustomModel, self).__init__()\n",
        "\n",
        "    self.D_in = input_size\n",
        "    self.H = hidden_size\n",
        "    self.D_out = output_size\n",
        "\n",
        "    self.linear = nn.Linear(self.D_in, self.H) \n",
        "    self.relu = nn.ReLU()\n",
        "    self.linear2 = nn.Linear(self.H, self.D_out)\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "  def forward(self, x):\n",
        "    linear = self.linear(x)\n",
        "    relu = self.relu(linear)\n",
        "    linear2 = self.linear2(relu)\n",
        "    output = self.sigmoid(linear2)\n",
        "    return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2cviDWPo8em"
      },
      "source": [
        "Let's do a foward pass through our model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0AWL-mPipANO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f2f6b54-82de-42b5-e06f-4e3ad30bfc86"
      },
      "source": [
        "# Sample input\n",
        "input = torch.randn(50, 10)\n",
        "# Initialize our model\n",
        "model = CustomModel(input_size=10, hidden_size=5, output_size=3)\n",
        "# Forward pass through model (this calls the forward() method)\n",
        "# The output is a matrix of N x 3, where 3 is the number of \n",
        "# features in the output (e.g. 3-class classification)\n",
        "y_pred = model(input)\n",
        "\n",
        "print(f\"Model output: {y_pred}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model output: tensor([[0.4220, 0.4772, 0.5618],\n",
            "        [0.4114, 0.4716, 0.5666],\n",
            "        [0.4284, 0.4660, 0.5539],\n",
            "        [0.5676, 0.4454, 0.5301],\n",
            "        [0.5361, 0.4668, 0.4520],\n",
            "        [0.5370, 0.4934, 0.4697],\n",
            "        [0.6396, 0.4531, 0.5346],\n",
            "        [0.4993, 0.4860, 0.5049],\n",
            "        [0.5852, 0.4640, 0.4669],\n",
            "        [0.6652, 0.4917, 0.4158],\n",
            "        [0.4619, 0.4594, 0.5764],\n",
            "        [0.4753, 0.5040, 0.5377],\n",
            "        [0.4754, 0.4726, 0.5403],\n",
            "        [0.4455, 0.4638, 0.5658],\n",
            "        [0.4694, 0.4858, 0.4642],\n",
            "        [0.5891, 0.4623, 0.4335],\n",
            "        [0.5883, 0.5256, 0.4208],\n",
            "        [0.4852, 0.4465, 0.5768],\n",
            "        [0.5672, 0.5171, 0.4670],\n",
            "        [0.4866, 0.5052, 0.4919],\n",
            "        [0.6213, 0.4330, 0.5050],\n",
            "        [0.4521, 0.4562, 0.5715],\n",
            "        [0.5040, 0.4458, 0.5824],\n",
            "        [0.5356, 0.4772, 0.4922],\n",
            "        [0.4180, 0.4748, 0.5498],\n",
            "        [0.4705, 0.5018, 0.5226],\n",
            "        [0.6336, 0.4589, 0.4672],\n",
            "        [0.4435, 0.4857, 0.5515],\n",
            "        [0.4610, 0.4956, 0.5438],\n",
            "        [0.4646, 0.5000, 0.4968],\n",
            "        [0.4950, 0.5036, 0.5254],\n",
            "        [0.4437, 0.4734, 0.5469],\n",
            "        [0.4718, 0.4761, 0.5301],\n",
            "        [0.4425, 0.4740, 0.5781],\n",
            "        [0.5003, 0.4818, 0.5347],\n",
            "        [0.4816, 0.4930, 0.4906],\n",
            "        [0.4882, 0.4485, 0.5511],\n",
            "        [0.6128, 0.4528, 0.5317],\n",
            "        [0.5384, 0.4617, 0.5607],\n",
            "        [0.4272, 0.4728, 0.5725],\n",
            "        [0.4385, 0.4896, 0.4997],\n",
            "        [0.6037, 0.4661, 0.5804],\n",
            "        [0.5256, 0.5099, 0.4105],\n",
            "        [0.4581, 0.5308, 0.4412],\n",
            "        [0.4546, 0.4820, 0.5697],\n",
            "        [0.4404, 0.4785, 0.5688],\n",
            "        [0.4268, 0.4728, 0.5723],\n",
            "        [0.5080, 0.4910, 0.5466],\n",
            "        [0.4694, 0.4912, 0.4838],\n",
            "        [0.4796, 0.4765, 0.4708]], grad_fn=<SigmoidBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ORAZtTSfqDwf"
      },
      "source": [
        " > 2. Computing a Loss\n",
        "\n",
        "\n",
        "Now that we have the outputs from the model, we can compute the loss between our model's predictions and the true labels. The `torch.nn` module has many standard loss functions. Here's an example using Cross Entropy Loss. Recall that the output from our model was an `Nx3` matrix, where `N` is the number of samples and 3 is the number of classes. We also need a `Nx1` tensor of ground-truth labels, where each element in the tensor is the class for that sample (i.e. a label in `[0, C-1]`)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VaEsjOdMrnmj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "247fa838-e854-4d91-c1fa-8fc501f2dbe6"
      },
      "source": [
        "target = torch.empty(50, dtype=torch.long).random_(3)\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "loss = loss_fn(y_pred, target)\n",
        "print(f\"Model loss: {loss}\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loss: 1.094733715057373\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVgMPS-Ys0E4"
      },
      "source": [
        "\n",
        "\n",
        "> Choosing an Optimizer\n",
        "\n",
        "The `torch.optim` package provides an easy to use interface for common optimization algorithms. Below is how you would define an optimizer, e.g. ADAM optimizer. When you initialize the optimizer, you pass in the parameters of the model that need to be updated every iteration. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VDls0NvttPwa"
      },
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmuPscbntbEf"
      },
      "source": [
        "\n",
        "\n",
        "> 3. Clearing the previous gradients \n",
        "\n",
        "Before you calculate your gradients and update your model parameters, you have to clear any gradients that are currently stored in the graph from previous steps. Otherwise, you will keep accumulating gradients over several time steps...that leads to trouble!!! This is a very common error when people first start coding with PyTorch, so be aware!\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5HypMUPwUoH"
      },
      "source": [
        "optimizer.zero_grad()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KmudGogztgVu"
      },
      "source": [
        "\n",
        "\n",
        "> 4. Computing gradients\n",
        "\n",
        "Now, we want to compute the gradients of the loss with respect to the parameters. This is all done behind the scenes, you just need to write one simple line:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NcDy0kFCxTZ7"
      },
      "source": [
        "loss.backward()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3uLkY3h6xVww"
      },
      "source": [
        "\n",
        "\n",
        "> 5. Updating model parameters \n",
        "\n",
        "Now that we've computed all the necessary gradients, let's update our model parameters by taking a step with our optimizer:\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7_tPnFrtFvi"
      },
      "source": [
        "optimizer.step()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nx_6Uc9WyLoW"
      },
      "source": [
        "And those are the fundamentel parts of training a neural network with `PyTorch`! Let's put it all together and see if we can learn something:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tikolfW11LRw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "130867a7-6ad7-4bf4-f3fe-ec109bb1a168"
      },
      "source": [
        "# Sample input\n",
        "\n",
        "# tensor of ones\n",
        "target = torch.ones((10,5), dtype=torch.float)\n",
        "# Add some noise to y to make our input --> let's see if we can recover y! \n",
        "input = target + torch.randn(target.shape)\n",
        "\n",
        "print(f\"Training data: {input}\")\n",
        "print(f\"Target data: {target}\")\n",
        "print(f\"Shape of training data: {input.shape}\")\n",
        "print(f\"Shape of training targets: {target.shape}\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data: tensor([[ 1.4522,  1.4338,  0.5959, -0.7123,  1.1290],\n",
            "        [ 1.0870,  0.4542,  0.6775,  0.1876,  1.6414],\n",
            "        [ 0.7295,  1.0679,  0.9739,  1.7429,  0.8815],\n",
            "        [ 1.5838, -1.0341,  0.5970, -0.8812,  1.1014],\n",
            "        [-0.3065,  2.2987, -0.4444,  1.9551,  1.5242],\n",
            "        [ 0.3595,  0.2741,  1.2791,  1.5211, -0.1085],\n",
            "        [ 1.8763, -0.1687,  1.3902,  1.6630,  1.2452],\n",
            "        [ 1.6329,  1.7060, -0.0257,  1.7950,  0.8797],\n",
            "        [ 1.1290,  2.5302,  0.4437,  0.4617,  0.9326],\n",
            "        [ 2.9515,  2.4048,  2.2147,  0.1978, -0.3741]])\n",
            "Target data: tensor([[1., 1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1., 1.]])\n",
            "Shape of training data: torch.Size([10, 5])\n",
            "Shape of training targets: torch.Size([10, 5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "020_LWQkyU4Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49bf4478-2a0c-4523-c240-d42b6052c751"
      },
      "source": [
        "# Initialize our model\n",
        "model = CustomModel(input_size=5, hidden_size=3, output_size=5)\n",
        "# Define our loss (Binary Cross Entropy Loss)\n",
        "loss_fn = nn.BCELoss()\n",
        "# Initialize our optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
        "# total number of epochs we want to train over\n",
        "epochs = 20\n",
        "# set the model to training mode\n",
        "model.train()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  # erase any stored gradients\n",
        "  optimizer.zero_grad()\n",
        "  # forward pass through model\n",
        "  y_pred = model(input)\n",
        "  # get the loss:\n",
        "  loss = loss_fn(y_pred, target)\n",
        "  # metrics: Mean Average Error\n",
        "  mean_average_error = torch.sum(torch.abs(y_pred - target)) # torch.nn.L1Loss()\n",
        "  # Compute the gradients\n",
        "  loss.backward()\n",
        "  # Print stats\n",
        "  # Notice here how I've printed `loss.item()` instead of print(loss)...this is very important!!!\n",
        "  # Otherwise, it will print the entire graph and this operation will build up over time...leads to memory errors!\n",
        "  print(f\"Epoch {epoch}: traing loss: {loss.item()} \\t Mean Average Error: {mean_average_error}\")\n",
        "\n",
        "  # Take a step to optimize the weights\n",
        "  optimizer.step()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: traing loss: 0.7005239725112915 \t Mean Average Error: 24.63335418701172\n",
            "Epoch 1: traing loss: 0.5813505053520203 \t Mean Average Error: 21.56714630126953\n",
            "Epoch 2: traing loss: 0.42759600281715393 \t Mean Average Error: 16.952749252319336\n",
            "Epoch 3: traing loss: 0.2639540433883667 \t Mean Average Error: 11.26090145111084\n",
            "Epoch 4: traing loss: 0.13721483945846558 \t Mean Average Error: 6.203320503234863\n",
            "Epoch 5: traing loss: 0.06214423105120659 \t Mean Average Error: 2.917966604232788\n",
            "Epoch 6: traing loss: 0.025949794799089432 \t Mean Average Error: 1.2489770650863647\n",
            "Epoch 7: traing loss: 0.01069071888923645 \t Mean Average Error: 0.5226072072982788\n",
            "Epoch 8: traing loss: 0.004489819519221783 \t Mean Average Error: 0.22160351276397705\n",
            "Epoch 9: traing loss: 0.0019510985584929585 \t Mean Average Error: 0.09685015678405762\n",
            "Epoch 10: traing loss: 0.0008827897836454213 \t Mean Average Error: 0.04396253824234009\n",
            "Epoch 11: traing loss: 0.0004167843144387007 \t Mean Average Error: 0.020792841911315918\n",
            "Epoch 12: traing loss: 0.00020540718105621636 \t Mean Average Error: 0.010257542133331299\n",
            "Epoch 13: traing loss: 0.00010563143587205559 \t Mean Average Error: 0.005277812480926514\n",
            "Epoch 14: traing loss: 5.6614528148202226e-05 \t Mean Average Error: 0.0028295516967773438\n",
            "Epoch 15: traing loss: 3.158637264277786e-05 \t Mean Average Error: 0.0015789270401000977\n",
            "Epoch 16: traing loss: 1.8312144675292075e-05 \t Mean Average Error: 0.0009154677391052246\n",
            "Epoch 17: traing loss: 1.1021958016499411e-05 \t Mean Average Error: 0.0005510449409484863\n",
            "Epoch 18: traing loss: 6.8633071350632235e-06 \t Mean Average Error: 0.00034314393997192383\n",
            "Epoch 19: traing loss: 4.430001808941597e-06 \t Mean Average Error: 0.00022149085998535156\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJ6wqbDb36dj"
      },
      "source": [
        "Let's see how our model does on new, unseen dataðŸ‡°\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDebZfjX4JKF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43f95f96-d263-43b0-e3ca-ebac84f8df2f"
      },
      "source": [
        "# Test set\n",
        "test_y = torch.ones((10,5), dtype=torch.float)\n",
        "test_x = test_y + torch.randn(test_y.shape)\n",
        "print(f\"Test set: {test_x}\")\n",
        "\n",
        "# Set the model to eval mode so we don't update any gradients! \n",
        "model.eval()\n",
        "# forward pass\n",
        "y_pred = model(test_x)\n",
        "mean_average_error = torch.sum(torch.abs(y_pred - test_y))\n",
        "\n",
        "print(f\"Model predictions on test set: {y_pred}\")\n",
        "print(f\"MAE on test set: {mean_average_error}\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test set: tensor([[ 0.5713,  2.9225,  1.7564,  2.0633,  2.1324],\n",
            "        [ 0.3882,  1.0137,  0.5670,  0.1869,  1.9320],\n",
            "        [ 3.2297,  2.5533,  1.9314, -0.5266, -0.0843],\n",
            "        [-1.0570,  2.8383,  2.5270,  1.9148,  0.4592],\n",
            "        [ 0.8176,  0.6290,  0.0762,  0.3075,  0.8088],\n",
            "        [ 0.3096,  2.6133,  2.2863,  0.5647,  3.1094],\n",
            "        [ 1.4065,  0.9189,  1.2627,  1.4747,  1.9253],\n",
            "        [ 1.9570,  0.2657, -0.5802,  0.3128,  0.8996],\n",
            "        [-0.8213,  1.6385,  0.1345,  0.4373,  1.7196],\n",
            "        [-0.4279,  0.7339,  0.7273,  1.4645,  1.2706]])\n",
            "Model predictions on test set: tensor([[1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
            "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
            "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
            "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
            "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
            "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
            "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
            "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
            "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
            "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
            "MAE on test set: 9.02414321899414e-05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vknj5oO4mdh"
      },
      "source": [
        "We're able to recover the noise from the data pretty well! \n",
        "\n",
        "That's it for the introduction to training Neural Networks with PyTorch. I highly recommend following this [60 Minute Blitz with PyTorch](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html) to gain a better understanding of PyTorch basics. Specifically, you should learn about PyTorch's Dataloaders next! "
      ]
    }
  ]
}